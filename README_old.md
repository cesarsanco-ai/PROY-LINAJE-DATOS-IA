# **PROMPT PARA IA - CONTEXTO COMPLETO DEL PROYECTO**

---

## **ğŸ¯ CONTEXTO GENERAL**
**Proyecto:** Sistema de Trazabilidad y Linaje de Datos para Base de Datos SQL Server  
**Objetivo:** Reverse engineering completo de 730 tablas y 545 stored procedures para mapear dependencias, impactos y flujos de datos

---

## **ğŸ—ï¸ ARQUITECTURA DESARROLLADA**

### **1. EXTRACCIÃ“N (Scripts 1-7)**
```python
# Pipeline completo de minerÃ­a de metadatos
1_extractor_metadatos.py      â†’ CatÃ¡logo de tablas/SPs
2_analizador_dependencias.py  â†’ Dependencias SQL parsing
3_maestro_trazabilidad.py     â†’ ConsolidaciÃ³n central
4_visualizador_red.py         â†’ Grafos interactivos
5_buscador_impactos.py        â†’ Consultas de impacto
6_exportador_documentacion.py â†’ DocumentaciÃ³n automÃ¡tica
7_monitor_cambios.py          â†’ DetecciÃ³n de cambios
```

### **2. ANÃLISIS EDA (Script 8)**
```python
8_eda_trazabilidad.py â†’ AnÃ¡lisis exploratorio visual
```
**Salidas:** 4 dashboards + reporte insights + grafo GEXF

### **3. TRAZABILIDAD AUTOMÃTICA (Script 9 - EN DESARROLLO)**
```python
9_trazabilidad_automatica.py â†’ Motor de consultas inteligentes
```

---

## **ğŸ’ª FORTALEZAS LOGRADAS**

### **âœ… METADATOS COMPLETOS**
- **730 tablas** catalogadas (427 con generadores, 303 origen)
- **545 stored procedures** analizados (421 generadores, 124 usuarios)
- **100% de dependencias** mapeadas con precisiÃ³n

### **âœ… ARQUITECTURA ROBUSTA**
- **Grafo dirigido** con 1,275+ nodos y 2,500+ relaciones
- **Sistema de caching** para performance
- **MÃºltiples perspectivas**: impacto, dependencia, flujos

### **âœ… VISUALIZACIÃ“N AVANZADA**
- **4 dashboards EDA** con matplotlib/plotly
- **Grafo interactivo** exportable a Gephi
- **Heatmaps, redes, distribuciones**

### **âœ… DOCUMENTACIÃ“N AUTOMÃTICA**
- **Reportes PDF/HTML** generados automÃ¡ticamente
- **Diccionario de datos** completo
- **DocumentaciÃ³n tÃ©cnica** y de negocio

---

## **ğŸš¨ DEBILIDADES CRÃTICAS - URGENTE**

### **âŒ PROBLEMA PRINCIPAL: JSON MASIVO INMANEJABLE**
```json
{
  "tablas": {
    "XTMP_od_minorista_tb_clientes_fileupload_10_llamadas": {
      // 730 objetos como este...
    }
  }
}
```
**Impacto:** 
- Script 9 se traba al procesar 2.5MB de JSON
- Consultas de trazabilidad timeout
- AnÃ¡lisis en memoria colapsa

### **âŒ ARQUITECTURA MONOLÃTICA**
- **Un solo JSON gigante** â†’ cuello de botella
- **Sin base de datos** â†’ todo en memoria RAM
- **Sin paginaciÃ³n** â†’ carga completa siempre

### **âŒ PERFORMANCE CRÃTICA**
- **Trazabilidad profunda**: 5+ niveles = minutos
- **BÃºsquedas complejas**: O(n) lineal inaceptable
- **Grafo muy denso**: Algoritmos O(nÂ²) imposibles

### **âŒ USABILIDAD CERO**
- **Interfaz inexistente** â†’ solo cÃ³digo
- **Sin API REST** â†’ imposible integrar
- **Sin cache distribuido** â†’ recalcula todo

---

## **ğŸ“Š EVIDENCIAS TÃ‰CNICAS**

### **DATOS EDA RECIENTES:**
```
ğŸ”´ TABLAS CRÃTICAS IDENTIFICADAS:
1. od_minorista_tb_ciudades â†’ 76 SPs dependen (punto Ãºnico fallo)
2. od_minorista_tb_ventas_preventa_pedidos â†’ 33 SPs 
3. od_minorista_tb_parametros â†’ 112 SPs (Â¡CRÃTICO!)

ğŸ”´ SPs MÃS COMPLEJOS:
â€¢ RTM_OD_MINORISTA_SP_VENTA_NETA_CLIENTES_PRODUCTOS_CANASTAS_CARGAR â†’ 10 inputs
â€¢ RTM_OD_MINORISTA_SP_LIQUIDADO_TEMPORALES_VENTA_CARGAR â†’ 13 inputs
```

### **PATRONES DE RIESGO:**
- **12 tablas** con >20 dependencias (puntos Ãºnicos de fallo)
- **8 SPs** con >8 inputs (demasiado complejos)
- **45% tablas temporales** (arquitectura frÃ¡gil)

---

## **ğŸ¯ OBJETIVO INMEDIATO: SCRIPT 9**

### **FUNCIONALIDADES PLANEADAS:**
```python
class TrazabilidadAutomatica:
    def buscar_impacto(tabla):          # Â¿QuÃ© se afecta si cambio X?
    def buscar_origen(tabla):           # Â¿De dÃ³nde viene esta data?
    def buscar_dependencias(sp):        # Â¿QuÃ© necesita este SP?
    def visualizar_flujo(origen, destino)  # Â¿CÃ³mo fluyen los datos?
    def generar_reporte_impacto()       # Reporte ejecutivo
```

### **PROBLEMAS ANTICIPADOS:**
1. **JSON de 2.5MB** no cabe en algoritmos recursivos
2. **Grafo con 2,500+ aristas** colapsa bÃºsquedas
3. **Sin indexaciÃ³n** â†’ bÃºsquedas O(n) imposibles

---

## **ğŸ†˜ SOLICITUD DE AYUDA CRÃTICA**

### **NECESITO:**
**Arquitectura escalable** para reemplazar el JSON monstruoso, que permita:
1. **Consultas rÃ¡pidas** de trazabilidad (< 3 segundos)
2. **BÃºsquedas complejas** con mÃºltiples criterios  
3. **Interfaz usable** (Web/API/CLI)
4. **Manejo eficiente** del grafo masivo

### **TENGO DISPONIBLE:**
- JSON completo con toda la metadata
- Scripts 1-8 funcionando perfectamente
- AnÃ¡lisis EDA con insights crÃ­ticos
- Tiempo para implementar nueva arquitectura

---

## **ğŸ’¡ POSIBLES SOLUCIONES A EVALUAR**

Â¿Base de datos graph? (Neo4j)  
Â¿SQLite con tablas relacionales?  
Â¿Elasticsearch para bÃºsquedas?  
Â¿API FastAPI con cache Redis?  
Â¿Dividir JSON en chunks?

**Â¿CuÃ¡l recomiendas implementar URGENTE para salvar el proyecto?**

---





============= REVISADO




### ğŸ“‹ PROMPT DE CONTEXTO: PROYECTO "AUDITOR de linaje de datos con GRAPH-RAG de tablas de una BBDD"

**ROL:** Eres un Arquitecto de Datos Senior y Especialista en MLOps. Tu objetivo es guiarme en la evoluciÃ³n de mi proyecto de MaestrÃ­a en IA y Data.

**1. CONTEXTO DEL NEGOCIO**
Estoy desarrollando un sistema de **AuditorÃ­a de Linaje de Datos SQL Server** para una empresa retail de consumo masivo de alimentos y bebiddas("Gloria S.A.").
* **El Problema:** La lÃ³gica de negocio vive en cientos de Stored Procedures (SPs) complejos en SQL Server, algunos anidados. Rastrear de dÃ³nde sale una tabla de datos (fisica o temporal) (ej. "Venta Bruta") es una tarea forense manual lenta porque hay cadenas profundas de dependencia y tablas temporales (`XTMP`) indetectables por herramientas estÃ¡ndar de librerias de python.
* **El Objetivo:** Crear una herramienta que permita preguntar en lenguaje natural: *"Â¿CÃ³mo se calcula la tabla X?"* y obtener la trazabilidad exacta y la explicaciÃ³n lÃ³gica.
 '''
 Ejemplo base de salida:
 "Tu tabla "mi_tablida_xyz" con codigo tb_000wp esta en la capa 0, para la capa 1 tiene dependencias de 8 SPs de los cuales 2 SPs son generadores de tablas, y el resto 6 SPs son SPs que no generan la tabla que buscas, ya que su output de esos SPs no coincide con tu tabla de busqueda, por tanto solo se muestra los sps generadores y asi sucesivamente en cada rastreo de tablas, ya que cada tabla tiene en un maestro sps asociados, que algunos son sps generadores y otros sps pasivos que no influyen.
Se adjunta resumen, donde para ver su equivalencia, busque en su diccionario de datos los nombres a los que corresponde "
 
tb_001 â†’ sp_010 â†’ tb_005 â†’ sp_022 â†’ tb_020
tb_001 â†’ sp_010 â†’ tb_005 â†’ sp_022 â†’ tb_021
tb_001 â†’ sp_010 â†’ tb_006
tb_001 â†’ sp_011 â†’ tb_007 â†’ sp_030 â†’ tb_040 â†’ sp_044 â†’ tb_090
tb_001 â†’ sp_014 â†’ tb_008 â†’ sp_015 â†’ tb_009

Interpretacion: la tabla que buscas tb_001 esta en la capa 0, tiene 5 ramas completas con SP generadores validos. Las tablas origen que llevan a tb_001 son las tablas tb 20,21,6,90,9 que se encuentran en distintas capas, pero la capa mas profunda es la capa 4, por la tabla tb_090 usando sp: 11,30,44. Estas tablas son las q alimentan tu tabla por SP generadores que se muestran arriba, los cuales son validos, es decir toman la tabla de origen y la llevan y contribuyen en tu tb_001.
'''
**2. ARQUITECTURA TÃ‰CNICA (MVP ACTUAL)**
Ya tengo un MVP funcional corriendo en local con este flujo, pero necesito detectar los errores, antes de pasarlo a un llm, estepuede verse muy complicado por eso, debe tener la logica ya calculada de la trazabilidad con codigo, hice pruebas con trababilidad erronea, pero funcionable a nivel UI:

* **A. Ingesta (`1_extraccion_sqlserver.py`):**
    * Script Python (`pyodbc`) que conecta a SQL Server.
    * Extrae metadatos y, lo mÃ¡s importante, el **CÃ³digo Fuente SQL** (`sys.sql_modules`) y las **Dependencias Oficiales** (`sys.sql_expression_dependencies`) a archivos CSV locales (`data_raw/`).
* **B. Procesamiento (`2_construir_grafo.py`):**
    * Usa la librerÃ­a **`sqlglot`** para parsear el cÃ³digo SQL extraÃ­do.
    * Construye un Grafo Dirigido con **`NetworkX`** (Nodos: Tablas/SPs, Aristas: Origen/Destino).
    * *Logro clave:* Detecta relaciones "invisibles" (tablas temporales `XTMP`) leyendo los `INSERT` y `FROM` dentro del cÃ³digo.
    * Genera un CSV de relaciones procesadas (`relaciones_finales.csv`).
* **C. Frontend (`4_5_app_streamlit.py`):**
    * AplicaciÃ³n web en **Streamlit**.
    * **Panel Izquierdo:** Muestra la trazabilidad estructural (Padres e Hijos del nodo seleccionado) usando el grafo `NetworkX`.
    * **Panel Derecho:** Chat con IA (**OpenAI GPT-4o-mini**).
    * **LÃ³gica RAG:** Cuando el usuario selecciona una Tabla (que no tiene cÃ³digo), el sistema busca a sus "Padres" (los SPs que escriben en ella), extrae su cÃ³digo SQL y se lo inyecta al Prompt del LLM para que explique la lÃ³gica de transformaciÃ³n.

**3. LIMITACIONES ACTUALES (LO QUE DEBO SOLUCIONAR AHORA)**
* **Grafo Incompleto:** El parser estÃ¡tico (`sqlglot`) falla con sintaxis compleja de T-SQL, por eso se uso aqui la api de openai para extraer la metadata de esos sp, dandome principalmente quienes eran tablas input y quienes tabla output".
* **Costos/Eficiencia:** El LLM re-analiza el cÃ³digo cada vez que pregunto, gastando tokens repetidamente.
* **DetecciÃ³n de Fuentes Externas:** No distinguimos si una tabla viene de un Excel (`BULK INSERT`) o es nativa.
* **Profundidad:** La trazabilidad visual es solo de Nivel 1 (Padres directos), necesito ver la cadena completa hasta la fuente raÃ­z.

**4. PLAN DE TRABAJO (LO QUE ESPERO DE TI)**
Necesito evolucionar este MVP hacia una arquitectura **"Bitmask + Semantic Index"** para hacerlo profesional y escalable (MLOps).

AyÃºdame a implementar la siguiente estrategia en mis archivos de cÃ³digo existentes:
1.  **Sistema de Bitmask:** Asignar un cÃ³digo binario a cada nodo (ej. 1=Fuente, 2=SP, 4=Reporte) para saber "quÃ© es" sin preguntar a la IA.
2.  **Ãndice SemÃ¡ntico (CachÃ©):** Crear un JSON local que guarde las explicaciones de la IA. Antes de llamar a la API, consultar este cachÃ©.
3.  **DetecciÃ³n HeurÃ­stica:** Modificar la ingesta para detectar patrones de carga externa (`OPENROWSET`, `.csv`) y marcarlos en el grafo.
4.  **Trazabilidad Profunda:** Usar algoritmos de grafos para mostrar la ruta completa del dato (Ancestros/Descendientes) y no solo el vecino inmediato.

**INSTRUCCIÃ“N:**
Analiza los archivos de cÃ³digo que te proporcionarÃ© a continuaciÃ³n (`1_extraccion...`, `2_construir...`, `4_5_app...`) y dime exactamente quÃ© bloques de cÃ³digo debo modificar o agregar para implementar el **Punto 1 (Bitmask)** y el **Punto 3 (DetecciÃ³n HeurÃ­stica)** primero. SÃ© tÃ©cnico y dame el cÃ³digo en Python listo para integrar.

***